{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "def re_remove_punct(s):  # From Vinko's solution, with fix.\n",
    "    return regex.sub('', s)\n",
    "\n",
    "def shingles(s, k, res = 'result ' + time.strftime(\"%Y%m%d-%H%M%S\") + '.txt'):\n",
    "    f = open(res, 'a')\n",
    "    while True:\n",
    "        result = (s[:k], s[1:])\n",
    "        s = result[1]\n",
    "        if result[0] == '':\n",
    "            break\n",
    "        f.write(result[0] + '\\n')\n",
    "    f.close()\n",
    "\n",
    "f = open('wasteland.txt', 'r')\n",
    "waste = f.read()\n",
    "f.close()\n",
    "\n",
    "waste = waste.replace('\\n',' ')\n",
    "waste = waste.replace('“', '')\n",
    "waste = waste.replace('”', '')\n",
    "waste = waste.replace('’', '')\n",
    "waste = re_remove_punct(waste)\n",
    "waste = waste.lower()\n",
    "waste\n",
    "\n",
    "waste_list = []\n",
    "\n",
    "test = 'a bc defg hij k && lmno pqr stuv wxy z'  \n",
    "        \n",
    "# shingles(waste, 9, 'waste_result.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "f = open('wasteland.txt', 'r')\n",
    "waste = f.read()\n",
    "f.close()\n",
    "\n",
    "waste = waste.replace('“', '')\n",
    "waste = waste.replace('”', '')\n",
    "waste = waste.replace('’', '')\n",
    "waste = re_remove_punct(waste)\n",
    "waste = waste.lower()\n",
    "\n",
    "f = open('wasteland2.txt', 'r')\n",
    "waste2 = f.read()\n",
    "f.close()\n",
    "\n",
    "waste2 = waste2.replace('“', '')\n",
    "waste2 = waste2.replace('”', '')\n",
    "waste2 = waste2.replace('’', '')\n",
    "waste2 = re_remove_punct(waste2)\n",
    "waste2 = waste2.lower()\n",
    "\n",
    "# print(waste2[:1000])\n",
    "\n",
    "print(Simhash(waste).distance(Simhash(waste2)))\n",
    "print(Simhash('1').distance(Simhash('2')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(Simhash(\"a bc defg hij k && lmno pqr stuv wxy z\").distance(Simhash(\"a bc defg hij k && lmno pqr stuv wxy zaaaaaaaa\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e77e5518405e7c6593c285c843c26c48\n",
      "307707622356704432759126517609369070664\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(int('001000001',2))\n",
    "# print(1<<2)\n",
    "# def bin(s):\n",
    "#     return str(s) if s<=1 else bin(s>>1) + str(s&1)\n",
    "\n",
    "# print(bin(12))\n",
    "# import hashlib\n",
    "\n",
    "# hashed = hashlib.md5(\"shingle\".encode('utf-8')).hexdigest()\n",
    "# hashed2 = hashlib.md5(\"shingles\".encode('utf-8')).hexdigest()\n",
    "# print(hashed)\n",
    "# print(hashed2)\n",
    "# print(' '.join(map(bin,bytearray(hashed,'utf8'))))\n",
    "# print()\n",
    "# print(' '.join(map(bin,bytearray(hashed2,'utf8'))))\n",
    "# int(hashed2,16)\n",
    "# int(hashlib.md5('yoyo'.encode('utf-8')).hexdigest(), 16)\n",
    "\n",
    "\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "array = os.urandom(1 << 20)\n",
    "# print(array)\n",
    "md5 = hashlib.md5()\n",
    "md5.update(array)\n",
    "digest = md5.hexdigest()\n",
    "print(digest)\n",
    "number = int(digest, 16)\n",
    "print(number)\n",
    "\n",
    "print(number % 1)\n",
    "6 ^ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false,
    "hideCode": false,
    "hideOutput": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', <__main__.Simhash object at 0x00000180C83D2B70>), ('2', <__main__.Simhash object at 0x00000180C83D2B38>), ('3', <__main__.Simhash object at 0x00000180C83D2668>)]\n",
      "index bucket size: 11\n",
      "index.get near dups: ['1']\n",
      "index.get near dups: ['2', '1']\n",
      "index.get_keys(s1): ['7a14: 0', '91c3: 1', 'e5cd: 2', '6988: 3']\n",
      "e5cd: 2\n",
      "{'6988e5cd91837a14,1', '6988e5cd91c37a14,2'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'6988e5cd91837a14'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# from simhash import Simhash, SimhashIndex\n",
    "def get_features(s):\n",
    "    width = 3\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[^\\w]+', '', s)\n",
    "    return [s[i:i + width] for i in range(max(len(s) - width + 1, 1))]\n",
    "\n",
    "data = {\n",
    "    1: u'How are you? I Am fine. blar blar blar blar blar Thanks.',\n",
    "    2: u'How are you i am fine. blar blar blar blar blar than',\n",
    "    3: u'This is simhash test.',\n",
    "}\n",
    "\n",
    "# print([(str(index), string) for index, string in data.items()])\n",
    "\n",
    "objs = [(str(index), Simhash(get_features(string))) for index, string in data.items()]\n",
    "print(objs) # list of tuples where the tuples are the index of the dictionary objs and the values are the SimHash'd objects\n",
    "index = SimhashIndex(objs, k=3)\n",
    "\n",
    "print(\"index bucket size:\", index.bucket_size())\n",
    "\n",
    "s1 = Simhash(get_features(u'How are you i am fine. blar blar blar blar blar thank'))\n",
    "print(\"index.get near dups:\", index.get_near_dups(s1))\n",
    "\n",
    "index.add('2', s1)\n",
    "print(\"index.get near dups:\", index.get_near_dups(s1))\n",
    "print('index.get_keys(s1):', list(index.get_keys(s1)))\n",
    "\n",
    "#what is \"get_keys(s1)\" returning?\n",
    "# print('offset:', index.offsets)\n",
    "# print('f:', index.f)\n",
    "\n",
    "# m = 2**index.offsets[1]-index.offsets[0]-1\n",
    "# print(m)\n",
    "# print(index.offsets[0])\n",
    "# print(index.offsets[0] & m)\n",
    "# m = 2** (4-48)-1\n",
    "# print(index.offsets[3] & m)\n",
    "print(list(index.get_keys(s1))[2])\n",
    "print(index.bucket[list(index.get_keys(s1))[3]])\n",
    "# print(get_features(data[1]))\n",
    "# sim2, obj_id = \n",
    "dups = index.bucket[list(index.get_keys(s1))[3]]\n",
    "for dup in dups:\n",
    "    sim2, obj_id = dup.split(',',1)\n",
    "    break\n",
    "sim2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimHash Code (Shift-E to unhide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "hideCode": true,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Created by 1e0n in 2013\n",
    "from __future__ import division, unicode_literals\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import hashlib\n",
    "import logging\n",
    "import collections\n",
    "from itertools import groupby\n",
    "\n",
    "if sys.version_info[0] >= 3:\n",
    "    basestring = str\n",
    "    unicode = str\n",
    "    long = int\n",
    "else:\n",
    "    range = xrange\n",
    "\n",
    "\n",
    "def _hashfunc(x):\n",
    "    return int(hashlib.md5(x).hexdigest(), 16)\n",
    "\n",
    "\n",
    "class Simhash(object):\n",
    "\n",
    "    def __init__(self, value, f=64, reg=r'[\\w\\u4e00-\\u9fcc]+', hashfunc=None):\n",
    "        \"\"\"\n",
    "        `f` is the dimensions of fingerprints\n",
    "        `reg` is meaningful only when `value` is basestring and describes\n",
    "        what is considered to be a letter inside parsed string. Regexp\n",
    "        object can also be specified (some attempt to handle any letters\n",
    "        is to specify reg=re.compile(r'\\w', re.UNICODE))\n",
    "        `hashfunc` accepts a utf-8 encoded string and returns a unsigned\n",
    "        integer in at least `f` bits.\n",
    "        \"\"\"\n",
    "\n",
    "        self.f = f\n",
    "        self.reg = reg\n",
    "        self.value = None\n",
    "\n",
    "        if hashfunc is None:\n",
    "            self.hashfunc = _hashfunc\n",
    "        else:\n",
    "            self.hashfunc = hashfunc\n",
    "\n",
    "        if isinstance(value, Simhash):\n",
    "            self.value = value.value\n",
    "        elif isinstance(value, basestring):\n",
    "            self.build_by_text(unicode(value))\n",
    "        elif isinstance(value, collections.Iterable):\n",
    "            self.build_by_features(value)\n",
    "        elif isinstance(value, long):\n",
    "            self.value = value\n",
    "        else:\n",
    "            raise Exception('Bad parameter with type {}'.format(type(value)))\n",
    "\n",
    "    def _slide(self, content, width=4):\n",
    "        return [content[i:i + width] for i in range(max(len(content) - width + 1, 1))]\n",
    "\n",
    "    def _tokenize(self, content):\n",
    "        content = content.lower()\n",
    "        content = ''.join(re.findall(self.reg, content))\n",
    "        ans = self._slide(content)\n",
    "        return ans\n",
    "\n",
    "    def build_by_text(self, content):\n",
    "        features = self._tokenize(content)\n",
    "        features = {k:sum(1 for _ in g) for k, g in groupby(sorted(features))}\n",
    "        return self.build_by_features(features)\n",
    "\n",
    "    def build_by_features(self, features):\n",
    "        \"\"\"\n",
    "        `features` might be a list of unweighted tokens (a weight of 1\n",
    "                   will be assumed), a list of (token, weight) tuples or\n",
    "                   a token -> weight dict.\n",
    "        \"\"\"\n",
    "        v = [0] * self.f\n",
    "        masks = [1 << i for i in range(self.f)]\n",
    "        if isinstance(features, dict):\n",
    "            features = features.items()\n",
    "        for f in features:\n",
    "            if isinstance(f, basestring):\n",
    "                h = self.hashfunc(f.encode('utf-8'))\n",
    "                w = 1\n",
    "            else:\n",
    "                assert isinstance(f, collections.Iterable)\n",
    "                h = self.hashfunc(f[0].encode('utf-8'))\n",
    "                w = f[1]\n",
    "            for i in range(self.f):\n",
    "                v[i] += w if h & masks[i] else -w\n",
    "        ans = 0\n",
    "        for i in range(self.f):\n",
    "            if v[i] > 0:\n",
    "                ans |= masks[i]\n",
    "        self.value = ans\n",
    "\n",
    "    def distance(self, another):\n",
    "        assert self.f == another.f\n",
    "        x = (self.value ^ another.value) & ((1 << self.f) - 1)\n",
    "        ans = 0\n",
    "        while x:\n",
    "            ans += 1\n",
    "            x &= x - 1\n",
    "        return ans\n",
    "\n",
    "\n",
    "class SimhashIndex(object):\n",
    "\n",
    "    def __init__(self, objs, f=64, k=2):\n",
    "        \"\"\"\n",
    "        `objs` is a list of (obj_id, simhash)\n",
    "        obj_id is a string, simhash is an instance of Simhash\n",
    "        `f` is the same with the one for Simhash\n",
    "        `k` is the tolerance\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.f = f\n",
    "        count = len(objs)\n",
    "        logging.info('Initializing %s data.', count)\n",
    "\n",
    "        self.bucket = collections.defaultdict(set)\n",
    "\n",
    "        for i, q in enumerate(objs):\n",
    "            if i % 10000 == 0 or i == count - 1:\n",
    "                logging.info('%s/%s', i + 1, count)\n",
    "\n",
    "            self.add(*q)\n",
    "\n",
    "    def get_near_dups(self, simhash):\n",
    "        \"\"\"\n",
    "        `simhash` is an instance of Simhash\n",
    "        return a list of obj_id, which is in type of str\n",
    "        \"\"\"\n",
    "        assert simhash.f == self.f\n",
    "\n",
    "        ans = set()\n",
    "\n",
    "        for key in self.get_keys(simhash):\n",
    "            dups = self.bucket[key]\n",
    "            logging.debug('key:%s', key)\n",
    "            if len(dups) > 200:\n",
    "                logging.warning('Big bucket found. key:%s, len:%s', key, len(dups))\n",
    "\n",
    "            for dup in dups:\n",
    "                sim2, obj_id = dup.split(',', 1)\n",
    "                sim2 = Simhash(long(sim2, 16), self.f)\n",
    "\n",
    "                d = simhash.distance(sim2)\n",
    "                if d <= self.k:\n",
    "                    ans.add(obj_id)\n",
    "        return list(ans)\n",
    "\n",
    "    def add(self, obj_id, simhash):\n",
    "        \"\"\"\n",
    "        `obj_id` is a string\n",
    "        `simhash` is an instance of Simhash\n",
    "        \"\"\"\n",
    "        assert simhash.f == self.f\n",
    "\n",
    "        for key in self.get_keys(simhash):\n",
    "            v = '%x,%s' % (simhash.value, obj_id)\n",
    "            self.bucket[key].add(v)\n",
    "\n",
    "    def delete(self, obj_id, simhash):\n",
    "        \"\"\"\n",
    "        `obj_id` is a string\n",
    "        `simhash` is an instance of Simhash\n",
    "        \"\"\"\n",
    "        assert simhash.f == self.f\n",
    "\n",
    "        for key in self.get_keys(simhash):\n",
    "            v = '%x,%s' % (simhash.value, obj_id)\n",
    "            if v in self.bucket[key]:\n",
    "                self.bucket[key].remove(v)\n",
    "\n",
    "    @property\n",
    "    def offsets(self):\n",
    "        \"\"\"\n",
    "        You may optimize this method according to <http://www.wwwconference.org/www2007/papers/paper215.pdf>\n",
    "        \"\"\"\n",
    "        return [self.f // (self.k + 1) * i for i in range(self.k + 1)]\n",
    "\n",
    "    def get_keys(self, simhash):\n",
    "        for i, offset in enumerate(self.offsets):\n",
    "            if i == (len(self.offsets) - 1):\n",
    "                m = 2 ** (self.f - offset) - 1\n",
    "            else:\n",
    "                m = 2 ** (self.offsets[i + 1] - offset) - 1\n",
    "            c = simhash.value >> offset & m\n",
    "            yield '%x: %x' % (c, i)\n",
    "\n",
    "    def bucket_size(self):\n",
    "        return len(self.bucket)\n",
    "    \n",
    "    def print_objs(self):\n",
    "        return None\n",
    "    \n",
    "    def print_buckets(self):\n",
    "        return self.bucket\n",
    "        \n",
    "    def print_f(self):\n",
    "        return self.f\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of SimHash Code\n",
    "\n",
    "# Pre-Processing Code (Shift-E to unhide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "hideCode": false,
    "hideOutput": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "test = 'a bc defg hij k && lmno pqr stuv \\t wxy z \\n abcd nate is the  ;best  '  \n",
    "\n",
    "def re_remove_punct(s):  # From Vinko's solution, with fix.\n",
    "    return regex.sub('', s)\n",
    "\n",
    "def remove_spaces(s, extra = False):\n",
    "    s = s.lower()\n",
    "    s = re_remove_punct(s)\n",
    "    s = \" \".join(s.split())\n",
    "    if extra == True:\n",
    "        s = s.replace(' ','')\n",
    "    return s\n",
    "\n",
    "def create_shingled_features(s, k, clean = False, remove_all_spaces= False, print_to_file = False, res = 'result ' + time.strftime(\"%Y%m%d-%H%M%S\") + '.txt'):\n",
    "    output = []\n",
    "    if clean == True:\n",
    "        s = remove_spaces(s, extra = remove_all_spaces)\n",
    "    f = open(res, 'a')\n",
    "    while True:\n",
    "        result = (s[:k], s[1:])\n",
    "        s = result[1]\n",
    "        if result[0] == '':\n",
    "            break\n",
    "        if print_to_file == True:\n",
    "            f.write(result[0] + '\\n')\n",
    "        else:\n",
    "            output.append(result[0])\n",
    "    f.close()\n",
    "    if print_to_file == True:\n",
    "        print(\"File printed to '\" + res + \"'\")\n",
    "    else:\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false,
    "hideCode": false,
    "hideOutput": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "#### import Shari's plagiarized samples\n",
    "\n",
    "import glob\n",
    "path=\"C:\\\\Users\\\\nathan.a.miller\\\\OneDrive - Accenture Federal Services\\\\USCIS\\\\Text\\\\Plagiarism Sample Text\\\\corpus-20090418\\\\*.txt\"\n",
    "chop = \"C:\\\\Users\\\\nathan.a.miller\\\\OneDrive - Accenture Federal Services\\\\USCIS\\\\Text\\\\Plagiarism Sample Text\\\\corpus-20090418\\\\\"\n",
    "\n",
    "plag = {} #store Shari's sample plagiarism files here\n",
    "\n",
    "for filename in glob.iglob(path):\n",
    "    chopt = filename.replace(chop,'')\n",
    "    with open(filename, encoding=\"ISO-8859-1\") as f:\n",
    "        plag[chopt] = f.read()\n",
    "    \n",
    "keys = list(plag.keys())\n",
    "\n",
    "### Create the index of simhashed files\n",
    "\n",
    "objs = [(str(k), Simhash(create_shingled_features(s, 3, clean=True))) for k, s in plag.items()]\n",
    "index = SimhashIndex(objs, k=3)\n",
    "\n",
    "simhasht_plags = {}\n",
    "\n",
    "for i in objs:\n",
    "    simhasht_plags[i[0]] = i[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import and simhash the original files\n",
    "\n",
    "import glob\n",
    "path=\"C:\\\\Users\\\\nathan.a.miller\\\\OneDrive - Accenture Federal Services\\\\USCIS\\\\Text\\\\Plagiarism Sample Text\\\\originals\\\\*.txt\"\n",
    "chop = \"C:\\\\Users\\\\nathan.a.miller\\\\OneDrive - Accenture Federal Services\\\\USCIS\\\\Text\\\\Plagiarism Sample Text\\\\originals\\\\\"\n",
    "\n",
    "originals = {} #store Shari's sample original files here\n",
    "simhasht = {}\n",
    "\n",
    "for filename in glob.iglob(path):\n",
    "    chopt = filename.replace(chop,'')\n",
    "    with open(filename, encoding=\"ISO-8859-1\") as f:\n",
    "        originals[chopt] = f.read()\n",
    "    simhasht[chopt] = Simhash(create_shingled_features(originals[chopt], 3, clean=True))\n",
    "    \n",
    "keys = list(originals.keys())\n",
    "\n",
    "\n",
    "# simhasht['orig_taska.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g0pE_taska.txt']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.get_near_dups(simhasht['orig_taska.txt'])\n",
    "# simhasht_plags['g0pE_taska.txt'].distance(simhasht['orig_taska.txt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "hideCode": false,
    "hideOutput": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328\n",
      "['g0pA_taska.txt']\n"
     ]
    }
   ],
   "source": [
    "# filename = 'C:\\\\Users\\\\nathan.a.miller\\\\Documents\\\\Jupyter\\\\Python\\\\g0pA_nate_taska.txt'\n",
    "path=\"C:\\\\Users\\\\nathan.a.miller\\\\OneDrive - Accenture Federal Services\\\\USCIS\\\\Text\\\\Plagiarism Sample Text\\\\corpus-20090418\\\\*.txt\"\n",
    "chop = \"C:\\\\Users\\\\nathan.a.miller\\\\OneDrive - Accenture Federal Services\\\\USCIS\\\\Text\\\\Plagiarism Sample Text\\\\corpus-20090418\\\\\"\n",
    "\n",
    "with open(filename, encoding=\"ISO-8859-1\") as f:\n",
    "        s1 = f.read()\n",
    "        \n",
    "s1 = Simhash(create_shingled_features(s1, 3, clean=True))\n",
    "\n",
    "print(index.bucket_size())\n",
    "print(index.get_near_dups(s1))\n",
    "l = index.get_keys(s1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Create hamming distance matrix\n",
    "\n",
    "#### import Shari's plagiarized samples\n",
    "\n",
    "import glob\n",
    "path=\"C:\\\\Users\\\\nathan.a.miller\\\\OneDrive - Accenture Federal Services\\\\USCIS\\\\Text\\\\Plagiarism Sample Text\\\\o+p\\\\*.txt\"\n",
    "chop = \"C:\\\\Users\\\\nathan.a.miller\\\\OneDrive - Accenture Federal Services\\\\USCIS\\\\Text\\\\Plagiarism Sample Text\\\\o+p\\\\\"\n",
    "\n",
    "all_dict = {} #store Shari's sample plagiarism files here\n",
    "\n",
    "for filename in glob.iglob(path):\n",
    "    chopt = filename.replace(chop,'')\n",
    "    with open(filename, encoding=\"ISO-8859-1\") as f:\n",
    "        all_dict[chopt] = f.read()\n",
    "    \n",
    "keys = list(all_dict.keys())\n",
    "\n",
    "### Create the index of simhashed files\n",
    "\n",
    "objs = [(str(k), Simhash(create_shingled_features(s, 3, clean=True))) for k, s in plag.items()]\n",
    "index = SimhashIndex(objs, k=3)\n",
    "\n",
    "all_simhasht = {}\n",
    "\n",
    "for i in objs:\n",
    "    all_simhasht[i[0]] = i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_simhasht)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
